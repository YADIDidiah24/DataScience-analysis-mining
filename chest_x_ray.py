# -*- coding: utf-8 -*-
"""chest-X-ray.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-Y7BIpSGyKZjAEYm6zsUZfbK35UnDTeS
"""

!pip install kaggle
!pip install tensorflow
!pip install pandas
!pip install setuptools
!pip install scikit-learn

!kaggle datasets download -d ashery/chexpert
!unzip chexpert.zip -d CheXpert-v1.0

import pandas as pd
import tensorflow as tf
import numpy as np
from tensorflow.keras.preprocessing.image import ImageDataGenerator

objects= pd.read_csv("/content/train_visualCheXbert.csv")

# List of label columns
LABEL_COLUMNS = ["Enlarged Cardiomediastinum", "Cardiomegaly", "Lung Opacity",
                 "Lung Lesion", "Edema", "Consolidation", "Pneumonia",
                 "Atelectasis", "Pneumothorax", "Pleural Effusion",
                 "Pleural Other", "Fracture", "Support Devices", "No Finding"]

def preprocess_image(img_path, img_size=(224, 224)):
    # Load the image from file path, resize, and normalize it
    img = tf.io.read_file(img_path)
    img = tf.image.decode_jpeg(img, channels=3)  # Decode JPEG image into RGB format
    img = tf.image.resize(img, img_size)  # Resize image
    img = img / 255.0  # Normalize pixel values
    return img
def load_image_and_labels(row):
    # Extract image path and labels
    img_path = row['Path']
    img = preprocess_image(img_path)

    # Extract labels as a tensor
    labels = tf.convert_to_tensor(row[LABEL_COLUMNS].values, dtype=tf.float32)

    return img, labels  # Return a tuple of (image, labels)

def create_dataset(dataframe, batch_size=32):
    # Create a TensorFlow Dataset from the dataframe
    image_paths = dataframe['Path'].values  # Extract image paths
    labels = dataframe[LABEL_COLUMNS].values.astype('float32')  # Convert labels to float32

    # Create TensorFlow Dataset
    dataset = tf.data.Dataset.from_tensor_slices((image_paths, labels))

    # Map the loading function to the dataset
    dataset = dataset.map(lambda img_path, label: (preprocess_image(img_path), label),
                          num_parallel_calls=tf.data.AUTOTUNE)

    # Shuffle and batch the dataset
    dataset = dataset.shuffle(buffer_size=len(dataframe), reshuffle_each_iteration=True)
    dataset = dataset.batch(batch_size)
    dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)  # Pre-fetch data for better performance

    return dataset

import tensorflow as tf
import pandas as pd
from sklearn.model_selection import train_test_split

objects_filtered = objects[LABEL_COLUMNS + ['Path']].copy()

# Handle NaN values by filling them with 0 or dropping rows with NaNs
objects_filtered.fillna(0, inplace=True)


train_df, val_df = train_test_split(objects_filtered, test_size=0.3, random_state=42)

# Reset index for the new DataFrames
train_df.reset_index(drop=True, inplace=True)
val_df.reset_index(drop=True, inplace=True)

import tensorflow as tf
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization
from tensorflow.keras.regularizers import l2
from tensorflow.keras.models import Sequential
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau

model = Sequential([
    Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 3), kernel_regularizer=l2(0.01)),
    BatchNormalization(),
    MaxPooling2D(pool_size=(2, 2)),

    Conv2D(64, (3, 3), activation='relu', kernel_regularizer=l2(0.01)),
    BatchNormalization(),
    MaxPooling2D(pool_size=(2, 2)),

    Conv2D(128, (3, 3), activation='relu', kernel_regularizer=l2(0.01)),
    BatchNormalization(),
    MaxPooling2D(pool_size=(2, 2)),

    Conv2D(256, (3, 3), activation='relu', kernel_regularizer=l2(0.01)),
    BatchNormalization(),
    MaxPooling2D(pool_size=(2, 2)),

    Conv2D(512, (3, 3), activation='relu', kernel_regularizer=l2(0.01)),
    BatchNormalization(),
    MaxPooling2D(pool_size=(2, 2)),

    Flatten(),

    Dense(256, activation='relu', kernel_regularizer=l2(0.01)),
    BatchNormalization(),
    Dense(128, activation='relu', kernel_regularizer=l2(0.01)),
    Dropout(0.5),

    Dense(len(LABEL_COLUMNS), activation='sigmoid')  # Sigmoid for multi-label classification
])
early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

from tensorflow.keras.preprocessing.image import ImageDataGenerator

# Initialize the ImageDataGenerator
train_datagen = ImageDataGenerator(rescale=1./255)
train_generator = train_datagen.flow_from_dataframe(
    dataframe=train_df,
    x_col="Path",
    y_col=LABEL_COLUMNS,
    target_size=(128, 128),
    batch_size=8,
    class_mode="raw"
)

# Initialize ImageDataGenerator for validation
val_datagen = ImageDataGenerator(rescale=1./255)

# Create validation generator
val_generator = val_datagen.flow_from_dataframe(
    dataframe=val_df,
    x_col="Path",
    y_col=LABEL_COLUMNS,
    target_size=(128, 128),  # Match with training generator size
    batch_size=8,
    class_mode="raw",
    shuffle=False  # No need to shuffle validation data
)

# Train the model with both generators
history = model.fit(
    train_generator,
    steps_per_epoch=len(train_df) // 8,
    validation_data=val_generator,
    validation_steps=len(val_df) // 8,
    epochs=5
)